{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2_Task2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPclPp4_cqLT"
      },
      "source": [
        "**BASELINE EXPERIMENT AS SUCH WITHOUT ANY CHANGE**\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQ3MqBFTcG5F",
        "outputId": "4c5254ef-50f5-4cb1-810b-04d5874299ea"
      },
      "source": [
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D\r\n",
        "from keras import backend as K\r\n",
        "  \r\n",
        "batch_size = 128\r\n",
        "num_classes = 10\r\n",
        "epochs = 12\r\n",
        "\r\n",
        "# input image dimensions\r\n",
        "img_rows, img_cols = 28, 28\r\n",
        "\r\n",
        "# the data, split between train and test sets\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "if K.image_data_format() == 'channels_first':\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n",
        "    input_shape = (1, img_rows, img_cols)\r\n",
        "else:\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
        "    input_shape = (img_rows, img_cols, 1)\r\n",
        "\r\n",
        "x_train = x_train.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "x_train /= 255\r\n",
        "x_test /= 255\r\n",
        "print('x_train shape:', x_train.shape)\r\n",
        "print(x_train.shape[0], 'training samples')\r\n",
        "print(x_test.shape[0], 'testing samples')\r\n",
        "\r\n",
        "# convert class vectors to binary class matrices\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(6, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(120, activation='relu'))\r\n",
        "model.add(Dense(84, activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "\r\n",
        "# https://keras.io/optimizers/ \r\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train, y_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=epochs,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=(x_test, y_test))\r\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print('Test loss:', score[0])\r\n",
        "print('Test accuracy:', score[1])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 21s 44ms/step - loss: 0.2888 - accuracy: 0.9098 - val_loss: 0.0859 - val_accuracy: 0.9724\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0720 - accuracy: 0.9775 - val_loss: 0.0544 - val_accuracy: 0.9802\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0514 - accuracy: 0.9838 - val_loss: 0.0493 - val_accuracy: 0.9837\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0406 - accuracy: 0.9873 - val_loss: 0.0443 - val_accuracy: 0.9861\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0339 - accuracy: 0.9890 - val_loss: 0.0428 - val_accuracy: 0.9865\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0282 - accuracy: 0.9912 - val_loss: 0.0445 - val_accuracy: 0.9853\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 21s 46ms/step - loss: 0.0236 - accuracy: 0.9923 - val_loss: 0.0359 - val_accuracy: 0.9879\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0203 - accuracy: 0.9936 - val_loss: 0.0415 - val_accuracy: 0.9868\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0176 - accuracy: 0.9944 - val_loss: 0.0347 - val_accuracy: 0.9895\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0149 - accuracy: 0.9954 - val_loss: 0.0398 - val_accuracy: 0.9877\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0134 - accuracy: 0.9954 - val_loss: 0.0388 - val_accuracy: 0.9891\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.0113 - accuracy: 0.9966 - val_loss: 0.0411 - val_accuracy: 0.9880\n",
            "Test loss: 0.041055675595998764\n",
            "Test accuracy: 0.9879999756813049\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oyW-4x2ENi8b"
      },
      "source": [
        "**Hypothesis 1: Decreasing number of Feature maps**\r\n",
        "\r\n",
        "Decreasing feature map to 1. The hypothesis is to check that it takes more than 1 convolution feature map to learn generalization on the given data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADhjFKHrNiV1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e136f08-06b3-4380-a71e-e4480664bd12"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_test.shape[0], 'testing samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "#Reducing only the number of feature maps\n",
        "model.add(Conv2D(1, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(1, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# https://keras.io/optimizers/ \n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.5674 - accuracy: 0.8141 - val_loss: 0.2854 - val_accuracy: 0.9037\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.2336 - accuracy: 0.9248 - val_loss: 0.1998 - val_accuracy: 0.9342\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 16s 34ms/step - loss: 0.1912 - accuracy: 0.9391 - val_loss: 0.1781 - val_accuracy: 0.9427\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.1684 - accuracy: 0.9456 - val_loss: 0.1685 - val_accuracy: 0.9450\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 17s 35ms/step - loss: 0.1533 - accuracy: 0.9500 - val_loss: 0.1602 - val_accuracy: 0.9484\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.1422 - accuracy: 0.9538 - val_loss: 0.1515 - val_accuracy: 0.9513\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.1339 - accuracy: 0.9563 - val_loss: 0.1454 - val_accuracy: 0.9543\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.1261 - accuracy: 0.9587 - val_loss: 0.1371 - val_accuracy: 0.9574\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 0.1193 - accuracy: 0.9607 - val_loss: 0.1437 - val_accuracy: 0.9540\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.1146 - accuracy: 0.9618 - val_loss: 0.1307 - val_accuracy: 0.9605\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.1092 - accuracy: 0.9651 - val_loss: 0.1365 - val_accuracy: 0.9570\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 0.1052 - accuracy: 0.9646 - val_loss: 0.1310 - val_accuracy: 0.9597\n",
            "Test loss: 0.13098829984664917\n",
            "Test accuracy: 0.9596999883651733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4jFXkaBNiAN"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "Accuracy decreases, which means that our hypothesis was correct that only one convolution features is not enough to represent the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5uI7KaWlcyJg"
      },
      "source": [
        "**Hypothesis 2 : Increasing number of Feature Maps**\r\n",
        "\r\n",
        "Increasing feature maps by 3 times. The hypothesis is to check that given that model can learn more number of features in convolution, can it learn features that are more relevant."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZByPpE8d2K7",
        "outputId": "501f7861-c0e6-4efb-d61c-7e6f823bdf8c"
      },
      "source": [
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "num_classes = 10\r\n",
        "epochs = 12\r\n",
        "\r\n",
        "# input image dimensions\r\n",
        "img_rows, img_cols = 28, 28\r\n",
        "\r\n",
        "# the data, split between train and test sets\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "if K.image_data_format() == 'channels_first':\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n",
        "    input_shape = (1, img_rows, img_cols)\r\n",
        "else:\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
        "    input_shape = (img_rows, img_cols, 1)\r\n",
        "\r\n",
        "x_train = x_train.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "x_train /= 255\r\n",
        "x_test /= 255\r\n",
        "print('x_train shape:', x_train.shape)\r\n",
        "print(x_train.shape[0], 'training samples')\r\n",
        "print(x_test.shape[0], 'testing samples')\r\n",
        "\r\n",
        "# convert class vectors to binary class matrices\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(18, kernel_size=(3, 3),activation='relu',input_shape=input_shape)) #INCREASED Feature Maps from 6 to 18\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Conv2D(48, (3, 3), activation='relu')) #Increased Feature maps from 16 to 48\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(120, activation='relu'))\r\n",
        "model.add(Dense(84, activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "\r\n",
        "# https://keras.io/optimizers/ \r\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train, y_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=epochs,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=(x_test, y_test))\r\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print('Test loss:', score[0])\r\n",
        "print('Test accuracy:', score[1])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 32s 68ms/step - loss: 0.2606 - accuracy: 0.9178 - val_loss: 0.0568 - val_accuracy: 0.9830\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 32s 68ms/step - loss: 0.0599 - accuracy: 0.9809 - val_loss: 0.0440 - val_accuracy: 0.9858\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 32s 67ms/step - loss: 0.0417 - accuracy: 0.9870 - val_loss: 0.0360 - val_accuracy: 0.9886\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 32s 67ms/step - loss: 0.0314 - accuracy: 0.9898 - val_loss: 0.0367 - val_accuracy: 0.9886\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 32s 67ms/step - loss: 0.0242 - accuracy: 0.9927 - val_loss: 0.0247 - val_accuracy: 0.9921\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 32s 68ms/step - loss: 0.0190 - accuracy: 0.9942 - val_loss: 0.0246 - val_accuracy: 0.9915\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 32s 69ms/step - loss: 0.0152 - accuracy: 0.9955 - val_loss: 0.0266 - val_accuracy: 0.9915\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 32s 68ms/step - loss: 0.0130 - accuracy: 0.9961 - val_loss: 0.0270 - val_accuracy: 0.9911\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 31s 67ms/step - loss: 0.0097 - accuracy: 0.9971 - val_loss: 0.0241 - val_accuracy: 0.9924\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 31s 67ms/step - loss: 0.0082 - accuracy: 0.9975 - val_loss: 0.0284 - val_accuracy: 0.9908\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 31s 67ms/step - loss: 0.0068 - accuracy: 0.9979 - val_loss: 0.0264 - val_accuracy: 0.9933\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 31s 67ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0271 - val_accuracy: 0.9927\n",
            "Test loss: 0.027100643143057823\n",
            "Test accuracy: 0.9926999807357788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJl3VqxPfduq"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "Hypothesis comes out to be true, that is, increasing number of feature maps may result in learning more relevant features, as accuracy increased from baseline accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcTFf5JCfecG"
      },
      "source": [
        "**Hypothesis 3: Kernel Size**\r\n",
        "\r\n",
        "We will only increase kernel size in this hypothesis, the idea behind this hypothesis is that our images in MNIST data are less complex, so it is possible that we can capture low level feature better in a kernal of bigger size. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUYyEqapffGs",
        "outputId": "22f78e83-f466-4795-84e0-67c787af7fb0"
      },
      "source": [
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "num_classes = 10\r\n",
        "epochs = 12\r\n",
        "\r\n",
        "# input image dimensions\r\n",
        "img_rows, img_cols = 28, 28\r\n",
        "\r\n",
        "# the data, split between train and test sets\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "if K.image_data_format() == 'channels_first':\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n",
        "    input_shape = (1, img_rows, img_cols)\r\n",
        "else:\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
        "    input_shape = (img_rows, img_cols, 1)\r\n",
        "\r\n",
        "x_train = x_train.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "x_train /= 255\r\n",
        "x_test /= 255\r\n",
        "print('x_train shape:', x_train.shape)\r\n",
        "print(x_train.shape[0], 'training samples')\r\n",
        "print(x_test.shape[0], 'testing samples')\r\n",
        "\r\n",
        "# convert class vectors to binary class matrices\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(6, kernel_size=(5, 5),activation='relu',input_shape=input_shape))  # KERNEL SIZE CHANGED FROM 3x3 to 5x5 \r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "model.add(Conv2D(16, (5, 5), activation='relu')) # KERNEL SIZE CHANGED FROM 3x3 to 5x5 \r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(120, activation='relu'))\r\n",
        "model.add(Dense(84, activation='relu'))\r\n",
        "\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "\r\n",
        "# https://keras.io/optimizers/ \r\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train, y_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=epochs,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=(x_test, y_test))\r\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print('Test loss:', score[0])\r\n",
        "print('Test accuracy:', score[1])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.2806 - accuracy: 0.9120 - val_loss: 0.0879 - val_accuracy: 0.9710\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0726 - accuracy: 0.9776 - val_loss: 0.0704 - val_accuracy: 0.9767\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0517 - accuracy: 0.9836 - val_loss: 0.0399 - val_accuracy: 0.9873\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0408 - accuracy: 0.9870 - val_loss: 0.0373 - val_accuracy: 0.9876\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0335 - accuracy: 0.9894 - val_loss: 0.0316 - val_accuracy: 0.9899\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0281 - accuracy: 0.9915 - val_loss: 0.0373 - val_accuracy: 0.9879\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0240 - accuracy: 0.9927 - val_loss: 0.0316 - val_accuracy: 0.9906\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0208 - accuracy: 0.9938 - val_loss: 0.0303 - val_accuracy: 0.9901\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 23s 48ms/step - loss: 0.0178 - accuracy: 0.9942 - val_loss: 0.0334 - val_accuracy: 0.9897\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0156 - accuracy: 0.9946 - val_loss: 0.0318 - val_accuracy: 0.9904\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0141 - accuracy: 0.9955 - val_loss: 0.0364 - val_accuracy: 0.9902\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 22s 48ms/step - loss: 0.0114 - accuracy: 0.9964 - val_loss: 0.0369 - val_accuracy: 0.9895\n",
            "Test loss: 0.036905501037836075\n",
            "Test accuracy: 0.9894999861717224\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbt-5CN1oZbD"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "We observe slight improvement over our baseline accuracy. This shows the 5x5 is a better choice in our dataset rather than widely used 3x3 , as our low level features are more extractable when we compare a 25 pixel at a time rather than 9."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "522H_5KUoaoh"
      },
      "source": [
        "**Hypothesis 4: Increasing nodes in non convolutional layers**\r\n",
        "\r\n",
        "In this hypothesis we want to increase nodes in last two layers which are non-convolutional layers. The reason is that more nodes can learn more complex representations, so there might be a better representation possible with more nodes, which can help in learning a better model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT7SXV88oa_t",
        "outputId": "e0972041-787a-42d8-c775-bc0033d8a4e1"
      },
      "source": [
        "import keras\r\n",
        "from keras.datasets import mnist\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Dropout, Flatten\r\n",
        "from keras.layers import Conv2D, MaxPooling2D\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "batch_size = 128\r\n",
        "num_classes = 10\r\n",
        "epochs = 12\r\n",
        "\r\n",
        "# input image dimensions\r\n",
        "img_rows, img_cols = 28, 28\r\n",
        "\r\n",
        "# the data, split between train and test sets\r\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\r\n",
        "\r\n",
        "if K.image_data_format() == 'channels_first':\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\r\n",
        "    input_shape = (1, img_rows, img_cols)\r\n",
        "else:\r\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\r\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\r\n",
        "    input_shape = (img_rows, img_cols, 1)\r\n",
        "\r\n",
        "x_train = x_train.astype('float32')\r\n",
        "x_test = x_test.astype('float32')\r\n",
        "x_train /= 255\r\n",
        "x_test /= 255\r\n",
        "print('x_train shape:', x_train.shape)\r\n",
        "print(x_train.shape[0], 'training samples')\r\n",
        "print(x_test.shape[0], 'testing samples')\r\n",
        "\r\n",
        "# convert class vectors to binary class matrices\r\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\r\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "model.add(Conv2D(6, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "\r\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\r\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dense(240, activation='relu'))  # increasing non convolution nodes from 120 to 240 \r\n",
        "model.add(Dense(168, activation='relu'))  # increasing non convolution nodes from 84 to 168\r\n",
        "\r\n",
        "model.add(Dense(num_classes, activation='softmax'))\r\n",
        "\r\n",
        "# https://keras.io/optimizers/ \r\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train, y_train,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=epochs,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=(x_test, y_test))\r\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\r\n",
        "print('Test loss:', score[0])\r\n",
        "print('Test accuracy:', score[1])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.2746 - accuracy: 0.9136 - val_loss: 0.0680 - val_accuracy: 0.9780\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0658 - accuracy: 0.9797 - val_loss: 0.0557 - val_accuracy: 0.9817\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0451 - accuracy: 0.9856 - val_loss: 0.0415 - val_accuracy: 0.9859\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0336 - accuracy: 0.9897 - val_loss: 0.0412 - val_accuracy: 0.9864\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0268 - accuracy: 0.9917 - val_loss: 0.0388 - val_accuracy: 0.9860\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0214 - accuracy: 0.9933 - val_loss: 0.0364 - val_accuracy: 0.9893\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0174 - accuracy: 0.9949 - val_loss: 0.0395 - val_accuracy: 0.9878\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0144 - accuracy: 0.9959 - val_loss: 0.0437 - val_accuracy: 0.9872\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0115 - accuracy: 0.9963 - val_loss: 0.0336 - val_accuracy: 0.9896\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0086 - accuracy: 0.9975 - val_loss: 0.0352 - val_accuracy: 0.9902\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 21s 46ms/step - loss: 0.0080 - accuracy: 0.9977 - val_loss: 0.0395 - val_accuracy: 0.9887\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 21s 45ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 0.0380 - val_accuracy: 0.9896\n",
            "Test loss: 0.037998415529727936\n",
            "Test accuracy: 0.9896000027656555\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk8-AzlZqv58"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "We observe slight improvement with respect to our baseline accuracy, so increasing complexity might be helping in learning a better representation. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63VCkRCd-YJm"
      },
      "source": [
        "**Hypothesis 5: Batch Size**\n",
        "\n",
        "Larger batch size allows computational speed using parallelization of GPUs. In general, too large of a batch size leads to poor generalization. For convex functions, using a batch equal to the entire dataset guarantees convergence to the global optima of the objective function but this costs a slower convergence to that optima. Using smaller batch have faster convergence to good solutions but not necessarily will reach to global optima. In this hypothesis we will test, if we increase batch size, keeping the epochs same, does the accuracy decrease, as it needs more epoch to generalize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QfT2TQ_-Yn3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb3bd502-ea52-4309-acfc-23e00e0eafa7"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 2048\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_test.shape[0], 'testing samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# https://keras.io/optimizers/ \n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "30/30 [==============================] - 15s 493ms/step - loss: 1.6412 - accuracy: 0.5022 - val_loss: 1.0183 - val_accuracy: 0.6332\n",
            "Epoch 2/12\n",
            "30/30 [==============================] - 15s 489ms/step - loss: 0.6144 - accuracy: 0.8034 - val_loss: 0.4716 - val_accuracy: 0.8587\n",
            "Epoch 3/12\n",
            "30/30 [==============================] - 15s 491ms/step - loss: 0.3204 - accuracy: 0.8968 - val_loss: 0.2756 - val_accuracy: 0.9119\n",
            "Epoch 4/12\n",
            "30/30 [==============================] - 15s 497ms/step - loss: 0.2037 - accuracy: 0.9383 - val_loss: 0.1690 - val_accuracy: 0.9486\n",
            "Epoch 5/12\n",
            "30/30 [==============================] - 15s 500ms/step - loss: 0.1735 - accuracy: 0.9462 - val_loss: 0.1467 - val_accuracy: 0.9554\n",
            "Epoch 6/12\n",
            "30/30 [==============================] - 15s 495ms/step - loss: 0.1411 - accuracy: 0.9566 - val_loss: 0.1252 - val_accuracy: 0.9597\n",
            "Epoch 7/12\n",
            "30/30 [==============================] - 15s 492ms/step - loss: 0.1250 - accuracy: 0.9617 - val_loss: 0.0974 - val_accuracy: 0.9700\n",
            "Epoch 8/12\n",
            "30/30 [==============================] - 15s 493ms/step - loss: 0.1166 - accuracy: 0.9641 - val_loss: 0.0903 - val_accuracy: 0.9718\n",
            "Epoch 9/12\n",
            "30/30 [==============================] - 15s 494ms/step - loss: 0.0887 - accuracy: 0.9732 - val_loss: 0.0803 - val_accuracy: 0.9756\n",
            "Epoch 10/12\n",
            "30/30 [==============================] - 15s 495ms/step - loss: 0.0841 - accuracy: 0.9741 - val_loss: 0.0725 - val_accuracy: 0.9768\n",
            "Epoch 11/12\n",
            "30/30 [==============================] - 15s 494ms/step - loss: 0.0798 - accuracy: 0.9757 - val_loss: 0.0857 - val_accuracy: 0.9734\n",
            "Epoch 12/12\n",
            "30/30 [==============================] - 15s 494ms/step - loss: 0.0716 - accuracy: 0.9781 - val_loss: 0.0663 - val_accuracy: 0.9798\n",
            "Test loss: 0.06625597178936005\n",
            "Test accuracy: 0.9797999858856201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sc-UcUv3YVB5"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "Since, Validation acc > training acc, we can run it for more epochs, this shows that larger batch size takes mores epochs to generalize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4d7cREtZD5Y"
      },
      "source": [
        "**Hypothesis 6: Decreasing Learning rate**\n",
        "\n",
        "Ideal learning rate is hard to predict, a slower learninig rate can increase compuational complexity whereas a large one can take us away from global optima.\n",
        "In this hypothesis, we will test with slower learning rate and show that in same number of epochs it will not converge, in other words it needs more epochs to converge. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsqksTGMZEHY",
        "outputId": "18759b13-b5df-4402-844a-a94f530b65d8"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_test.shape[0], 'testing samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# https://keras.io/optimizers/ \n",
        "#Decreased Learning rate\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(lr=0.1, rho=0.95, epsilon=None, decay=0.0),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.8261 - accuracy: 0.7588 - val_loss: 0.3012 - val_accuracy: 0.9123\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.2662 - accuracy: 0.9199 - val_loss: 0.2241 - val_accuracy: 0.9341\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.2101 - accuracy: 0.9366 - val_loss: 0.1867 - val_accuracy: 0.9442\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.1770 - accuracy: 0.9456 - val_loss: 0.1513 - val_accuracy: 0.9548\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.1512 - accuracy: 0.9538 - val_loss: 0.1419 - val_accuracy: 0.9560\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.1320 - accuracy: 0.9600 - val_loss: 0.1187 - val_accuracy: 0.9639\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 22s 46ms/step - loss: 0.1169 - accuracy: 0.9641 - val_loss: 0.1083 - val_accuracy: 0.9657\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 23s 48ms/step - loss: 0.1046 - accuracy: 0.9678 - val_loss: 0.0971 - val_accuracy: 0.9710\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0957 - accuracy: 0.9708 - val_loss: 0.0894 - val_accuracy: 0.9733\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0881 - accuracy: 0.9732 - val_loss: 0.0905 - val_accuracy: 0.9714\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0814 - accuracy: 0.9748 - val_loss: 0.0824 - val_accuracy: 0.9760\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 22s 47ms/step - loss: 0.0763 - accuracy: 0.9770 - val_loss: 0.0800 - val_accuracy: 0.9765\n",
            "Test loss: 0.08002486824989319\n",
            "Test accuracy: 0.9764999747276306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6IjbchwRroX"
      },
      "source": [
        "**Hypothesis 7: Increasing Learning rate**\r\n",
        "\r\n",
        "Ideal learning rate is hard to predict, a slower learninig rate can increase compuational complexity whereas a large one can take us away from global optima. In this hypothesis, we will test with higher learning rate and show that it  is taking us away from the solution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtbfmDLgRtff",
        "outputId": "d00c0df9-bcda-4c9b-c0df-38b70686513c"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_test.shape[0], 'testing samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# https://keras.io/optimizers/ \n",
        "#Increased Learning rate\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(lr=50.0, rho=0.95, epsilon=None, decay=0.0),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 2.3136 - accuracy: 0.1047 - val_loss: 2.3045 - val_accuracy: 0.1010\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 17s 35ms/step - loss: 2.3053 - accuracy: 0.1039 - val_loss: 2.3033 - val_accuracy: 0.1135\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 2.3048 - accuracy: 0.1069 - val_loss: 2.3045 - val_accuracy: 0.1135\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 2.3049 - accuracy: 0.1049 - val_loss: 2.3067 - val_accuracy: 0.1032\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 2.3051 - accuracy: 0.1054 - val_loss: 2.3031 - val_accuracy: 0.1135\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 2.3046 - accuracy: 0.1067 - val_loss: 2.3043 - val_accuracy: 0.1032\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 17s 35ms/step - loss: 2.3048 - accuracy: 0.1066 - val_loss: 2.3053 - val_accuracy: 0.0980\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 2.3049 - accuracy: 0.1067 - val_loss: 2.3034 - val_accuracy: 0.0982\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 2.3052 - accuracy: 0.1063 - val_loss: 2.3100 - val_accuracy: 0.1028\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 16s 34ms/step - loss: 2.3050 - accuracy: 0.1061 - val_loss: 2.3078 - val_accuracy: 0.1135\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 2.3051 - accuracy: 0.1062 - val_loss: 2.3086 - val_accuracy: 0.1135\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 16s 35ms/step - loss: 2.3053 - accuracy: 0.1044 - val_loss: 2.3119 - val_accuracy: 0.0958\n",
            "Test loss: 2.311934471130371\n",
            "Test accuracy: 0.0957999974489212\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuVHWIju-XI7"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "So far this is the least accuracy we have seen, higher learning rate is shifting us away from target vector. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZODsj7SGFY0"
      },
      "source": [
        "**Hypothesis 8: Decreasing nodes in non convolutional layers**\r\n",
        "\r\n",
        "In this hypothesis we want to decrease nodes in last two layers which are non-convolutional layers. The reason is that we want to check if convolutional layers itself can act as final features, without representing them again in higher dimension before the output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLvJ1xXhGFlQ",
        "outputId": "ac4ce0fd-74e1-43c7-ae73-69e4c59653b1"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_test.shape[0], 'testing samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(6, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Flatten())\n",
        "#reducing the density of classification phase to a very low value\n",
        "model.add(Dense(2, activation='relu'))\n",
        "model.add(Dense(2, activation='relu'))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# https://keras.io/optimizers/ \n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 17s 36ms/step - loss: 1.5669 - accuracy: 0.4141 - val_loss: 1.2175 - val_accuracy: 0.5487\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 1.1104 - accuracy: 0.5687 - val_loss: 1.0222 - val_accuracy: 0.6037\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 18s 37ms/step - loss: 0.9988 - accuracy: 0.6028 - val_loss: 0.9513 - val_accuracy: 0.6208\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 0.9458 - accuracy: 0.6209 - val_loss: 0.9178 - val_accuracy: 0.6345\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 0.9091 - accuracy: 0.6353 - val_loss: 0.8854 - val_accuracy: 0.6481\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 0.8825 - accuracy: 0.6475 - val_loss: 0.8678 - val_accuracy: 0.6601\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 0.8556 - accuracy: 0.6623 - val_loss: 0.8429 - val_accuracy: 0.6571\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 18s 37ms/step - loss: 0.8230 - accuracy: 0.6765 - val_loss: 0.8012 - val_accuracy: 0.6767\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 18s 37ms/step - loss: 0.7956 - accuracy: 0.6895 - val_loss: 0.7747 - val_accuracy: 0.6913\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 18s 37ms/step - loss: 0.7740 - accuracy: 0.6974 - val_loss: 0.7638 - val_accuracy: 0.6964\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 0.7569 - accuracy: 0.7089 - val_loss: 0.7482 - val_accuracy: 0.7223\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 0.7348 - accuracy: 0.7251 - val_loss: 0.7309 - val_accuracy: 0.7246\n",
            "Test loss: 0.7308602929115295\n",
            "Test accuracy: 0.7246000170707703\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzHHMTRuH12G"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "This shows that representing the output from convolutional layers in higher dimiensions is important to learn and use features from it, to train the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGVFLRPoMo7S"
      },
      "source": [
        "**Hypothesis 9: Changing optimizer to Adam**\r\n",
        "\r\n",
        "There are different types of gradient descent optimizers. In baseline code, we have used adadelta, which adapts learning rates based on a moving window of gradient updates. In this hypothesis we want to test Adam, which is based on the adaptive estimation of first-order and second-order moments. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPNsETAXMpVF",
        "outputId": "9a6d2463-75ef-4dc5-fd24-e62cb5e20236"
      },
      "source": [
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 12\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'training samples')\n",
        "print(x_test.shape[0], 'testing samples')\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(6, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "model.add(Conv2D(16, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(120, activation='relu'))\n",
        "model.add(Dense(84, activation='relu'))\n",
        "\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# https://keras.io/optimizers/ \n",
        "#Changed optimizer to Adam\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "              optimizer=keras.optimizers.Adam(lr=1.0),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 training samples\n",
            "10000 testing samples\n",
            "Epoch 1/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 3415.1353 - accuracy: 0.1040 - val_loss: 2.3293 - val_accuracy: 0.1010\n",
            "Epoch 2/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 2.3374 - accuracy: 0.1002 - val_loss: 2.3332 - val_accuracy: 0.0982\n",
            "Epoch 3/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 2.3430 - accuracy: 0.1005 - val_loss: 2.3361 - val_accuracy: 0.1135\n",
            "Epoch 4/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 2.3472 - accuracy: 0.1007 - val_loss: 2.3497 - val_accuracy: 0.1010\n",
            "Epoch 5/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 2.3493 - accuracy: 0.0986 - val_loss: 2.3425 - val_accuracy: 0.1010\n",
            "Epoch 6/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 2.3475 - accuracy: 0.1023 - val_loss: 2.3521 - val_accuracy: 0.1028\n",
            "Epoch 7/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 2.3459 - accuracy: 0.1006 - val_loss: 2.3359 - val_accuracy: 0.1032\n",
            "Epoch 8/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 2.3547 - accuracy: 0.1013 - val_loss: 2.3502 - val_accuracy: 0.1028\n",
            "Epoch 9/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 2.3443 - accuracy: 0.1029 - val_loss: 2.3106 - val_accuracy: 0.0974\n",
            "Epoch 10/12\n",
            "469/469 [==============================] - 17s 37ms/step - loss: 2.3489 - accuracy: 0.1027 - val_loss: 2.3467 - val_accuracy: 0.1010\n",
            "Epoch 11/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 2.3498 - accuracy: 0.1020 - val_loss: 2.3324 - val_accuracy: 0.1009\n",
            "Epoch 12/12\n",
            "469/469 [==============================] - 18s 38ms/step - loss: 2.3497 - accuracy: 0.1018 - val_loss: 2.3415 - val_accuracy: 0.1135\n",
            "Test loss: 2.341536283493042\n",
            "Test accuracy: 0.11349999904632568\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrG1onyYWkUs"
      },
      "source": [
        "**Remarks:**\r\n",
        "\r\n",
        "Hence, this optimizer is not working in this scenario."
      ]
    }
  ]
}